{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import json\n",
    "\n",
    "import typing as tp\n",
    "import faiss\n",
    "import shutil\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_csv('/Users/nadys/recsys_data/1600kfeedbacks.csv')\n",
    "track_metadata = pd.read_json('/Users/nadys/recsys_data/tracks.json', lines=True).drop_duplicates(subset=['track'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVES_THRESHOLD = 0.8\n",
    "NUM_TRACKS_THRESHOLD = 20\n",
    "\n",
    "NUM_NEGATIVE_SAMPLES = 10\n",
    "\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = data[data['time'] > POSITIVES_THRESHOLD].copy()\n",
    "track_counts = positives.groupby('track').size()\n",
    "\n",
    "# id треков, встретившихся хотя бы num_tracks_threshold раз\n",
    "tracks = set(track_counts[track_counts >= NUM_TRACKS_THRESHOLD].index.values) \n",
    "# данные о positives \n",
    "data_filt = positives[positives['track'].isin(tracks)]\n",
    "\n",
    "len(data_filt), len(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.pivot_table(data_filt, values='time', index='user', columns='track').fillna(0)\n",
    "# print('Interactions matrix: \\nshape=' + str(interactions.shape))\n",
    "# print('Sparsity=' + str((interactions != 0).values.sum() / interactions.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_metadata = track_metadata.fillna(value={'genre': 'Unk'})\n",
    "# track_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = pd.get_dummies(track_metadata[['genre']])\n",
    "item_features = pd.concat([track_metadata[['track', 'pop']], dummy_features], axis=1).set_index('track', drop=True)\n",
    "item_features['pop'] = np.log(item_features['pop'])\n",
    "item_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = data_filt[['user', 'track']].rename(columns={'track': 'track_pos'})\n",
    "triplets =  pd.concat([triplets] * NUM_NEGATIVE_SAMPLES).sort_index().reset_index(drop=True)\n",
    "triplets['track_neg'] = np.random.choice(range(50000), len(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdm = np.random.random(len(triplets))\n",
    "train_data = triplets[rdm < 0.9]\n",
    "val_data = triplets[rdm >= 0.9]\n",
    "\n",
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features.shape, interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def pad_with_specific_value(lst, size, val):\n",
    "    lst = list(set(lst))\n",
    "    shuffle(lst)\n",
    "    lst = lst[:size]\n",
    "    return np.pad(lst, (0, size - len(lst)), 'constant', constant_values=(val))\n",
    "\n",
    "padded_users = triplets.groupby('user').apply(lambda x: (\n",
    "    pad_with_specific_value(x['track_pos'].tolist(), 30, 50000).tolist()\n",
    "))\n",
    "\n",
    "padded_users = padded_users.reindex(range(10000), fill_value=[50000] * 30)\n",
    "padded_users = np.stack(padded_users.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSSMData(pl.LightningDataModule):\n",
    "  def __init__(self, train_triplets, val_triplets, test_triplets, item_features, padded_users):\n",
    "      super().__init__()\n",
    "      self.train_triplets = train_triplets\n",
    "      self.val_triplets = val_triplets\n",
    "      self.test_triplets = test_triplets\n",
    "      self.item_features = item_features\n",
    "      self.padded_users = padded_users\n",
    "\n",
    "  def _collect_data(self, triplets):\n",
    "      users = triplets['user'].values\n",
    "      positives = triplets['track_pos'].values\n",
    "      negatives = triplets['track_neg'].values\n",
    "\n",
    "      # Wipe out positive interacted tracks from user listen history\n",
    "      listened_tracks = self.padded_users[users]\n",
    "      listened_tracks[listened_tracks == positives.reshape(-1, 1)] = 50000\n",
    "      listened_tracks[listened_tracks == negatives.reshape(-1, 1)] = 50000\n",
    "\n",
    "      return td.TensorDataset(\n",
    "            torch.from_numpy(listened_tracks).long(),\n",
    "            torch.from_numpy(item_features.loc[positives].values.astype(np.float32)).double(),\n",
    "            torch.from_numpy(item_features.loc[negatives].values.astype(np.float32)).double()\n",
    "      )\n",
    "\n",
    "  def prepare_data(self, stage=None):\n",
    "      if stage == 'fit' or stage is None:\n",
    "        self.train_dataset = self._collect_data(self.train_triplets)\n",
    "        self.val_dataset = self._collect_data(self.val_triplets)\n",
    "      elif stage == 'test' or stage is None:\n",
    "        self.test_dataset = self._collect_data(self.test_triplets)\n",
    "\n",
    "  def train_dataloader(self):\n",
    "      return td.DataLoader(self.train_dataset, batch_size=2048, shuffle=True, num_workers=0)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "      return td.DataLoader(self.val_dataset, batch_size=2048, num_workers=0)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "      return td.DataLoader(self.test_dataset, batch_size=2048, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemNet(nn.Module):\n",
    "    def __init__(self, n_factors: int, dim_input: int, activation: tp.Callable[[torch.Tensor], torch.Tensor] = F.relu) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Linear(dim_input - 1, 32, bias=False)\n",
    "        self.dense_layer = nn.Linear(32 + 1, n_factors, bias=False)\n",
    "        self.output_layer = nn.Linear(n_factors + 32, n_factors, bias=False)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, item_features: torch.Tensor) -> torch.Tensor:\n",
    "        popularity = item_features[:, 0].view(-1, 1)\n",
    "        genre_emb = self.embedding_layer(item_features[:, 1:])\n",
    "\n",
    "        pop_genre = torch.concat([popularity, genre_emb], axis=1)\n",
    "        features = self.activation(self.dense_layer(pop_genre))\n",
    "\n",
    "        genre_features = torch.concat([genre_emb, features], axis=1)\n",
    "        output = self.output_layer(genre_features)\n",
    "        return output\n",
    "\n",
    "class UserNet(nn.Module):\n",
    "    def __init__(self, n_factors: int, num_embeddings: int, activation: tp.Callable[[torch.Tensor], torch.Tensor] = F.relu) -> None:\n",
    "        super().__init__()\n",
    "        self.track_embeddings = nn.EmbeddingBag(num_embeddings+1, n_factors, padding_idx=num_embeddings)\n",
    "        self.dense_layer = nn.Linear(n_factors, n_factors, bias=False)\n",
    "        self.output_layer = nn.Linear(n_factors + n_factors, n_factors, bias=False)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, user_tracks: torch.Tensor) -> torch.Tensor:\n",
    "        interactions_emb = self.track_embeddings(user_tracks)\n",
    "        features = self.activation(self.dense_layer(interactions_emb))\n",
    "        x = torch.concat([interactions_emb, features], axis=1)\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSSM(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_item_features: int,\n",
    "        item_number: int,\n",
    "        embedding_dim: int,\n",
    "        activation: tp.Callable[[torch.Tensor], torch.Tensor] = F.relu,\n",
    "        lr: float = 1e-3,\n",
    "        triplet_loss_margin: float = 0.4,\n",
    "        weight_decay: float = 1e-6,\n",
    "        log_to_prog_bar: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.triplet_loss_margin = triplet_loss_margin\n",
    "        self.weight_decay = weight_decay\n",
    "        self.log_to_prog_bar = log_to_prog_bar\n",
    "        self.item_net = ItemNet(embedding_dim, dim_item_features, activation)\n",
    "        self.user_net = UserNet(embedding_dim, item_number)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        user_ids: torch.Tensor,\n",
    "        item_features_pos: torch.Tensor,\n",
    "        item_features_neg: torch.Tensor,\n",
    "    ) -> tp.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        anchor = self.user_net(user_ids)\n",
    "        pos = self.item_net(item_features_pos)\n",
    "        neg = self.item_net(item_features_neg)\n",
    "\n",
    "        return anchor, pos, neg\n",
    "\n",
    "    def _step(self, batch, batch_idx, metric, prog_bar=False):\n",
    "        user_ids, pos, neg = batch\n",
    "        anchor, positive, negative = self(user_ids, pos, neg)\n",
    "        loss = F.triplet_margin_loss(anchor, positive, negative, margin=self.triplet_loss_margin)\n",
    "        self.log(metric, loss, prog_bar=prog_bar)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: tp.Sequence[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, batch_idx, 'train_loss')\n",
    "\n",
    "    def validation_step(self, batch: tp.Sequence[torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, batch_idx, 'val_loss', self.log_to_prog_bar)\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=False):\n",
    "        return self._step(batch, batch_idx, 'test_loss', self.log_to_prog_bar)\n",
    "\n",
    "    def inference(self, dataloader: td.DataLoader[tp.Any], mode: str = 'item') -> np.ndarray:\n",
    "        batches = []\n",
    "        user_ids = []\n",
    "        if(mode == 'user'):\n",
    "          model = self.user_net\n",
    "        elif(mode == 'item'):\n",
    "          model = self.item_net\n",
    "        else:\n",
    "          raise ValueError(f'Unsupported model {mode}!')\n",
    "\n",
    "        self.eval()\n",
    "        for batch in dataloader:\n",
    "            ids, features = batch\n",
    "            with torch.no_grad():\n",
    "                v_batch = model(features.to(self.device))\n",
    "            batches.append(v_batch)\n",
    "            user_ids.append(ids)\n",
    "        vectors = torch.cat(batches, dim=0).cpu().numpy()\n",
    "        vectors_ids = torch.cat(user_ids, dim=0).cpu().numpy()\n",
    "        return vectors_ids, vectors\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "        scheduler = {\n",
    "            'scheduler': lr_scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DSSMData(train_data, val_data, None, item_features, padded_users)\n",
    "net = DSSM(dim_item_features=23, item_number=50000, embedding_dim=EMBEDDING_DIM).double()\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=40,\n",
    "    accelerator='cpu',\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor='val_loss', patience=5),\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step'),\n",
    "        checkpoint_callback,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    net,\n",
    "    data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.move(checkpoint_callback.best_model_path, 'dssm_improved.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = DSSM.load_from_checkpoint(\"dssm_improved.ckpt\", dim_item_features=23, item_number=50000, embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = item_features.astype(np.float32).sort_index()\n",
    "items = torch.from_numpy(item_features.index.values)\n",
    "inf_items = torch.from_numpy(item_features.values).double()\n",
    "items_ds = td.TensorDataset(items, inf_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dl_items = td.DataLoader(items_ds, batch_size=128, shuffle=False, num_workers=1)\n",
    "track_ids, track_embeddings = best.double().inference(inf_dl_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.sort_index()\n",
    "user_ids_inf = torch.from_numpy(interactions.index.values)\n",
    "user_inf_feat = torch.from_numpy(padded_users[interactions.index.values]).long()\n",
    "user_ds = td.TensorDataset(user_ids_inf, user_inf_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dl_users = td.DataLoader(user_ds, batch_size=128, shuffle=False, num_workers=1)\n",
    "user_ids, user_embeddings = best.double().inference(inf_dl_users, \"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_embeddings.shape, user_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_res = faiss.StandardCpuResources()\n",
    "index_flat = faiss.index_factory(track_embeddings.shape[1], \"Flat\", faiss.METRIC_L2)\n",
    "\n",
    "#index = faiss.index_cpu_to_gpu(gpu_res, 0, index_flat)\n",
    "index = index_flat\n",
    "index.add(track_embeddings.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "with open(\"/Users/nadys/recsys_data/dssm_improved.json\", \"w\") as rf:\n",
    "    for user, user_emb in tqdm(zip(user_ids, user_embeddings), total=len(user_ids)):\n",
    "      dists, neighbours = index.search(user_emb.astype('float32')[np.newaxis, :], k)\n",
    "      recommendation = {\n",
    "        \"user\": int(user),\n",
    "        \"tracks\": neighbours.flatten().tolist()\n",
    "      }\n",
    "      rf.write(json.dumps(recommendation) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /Users/nadys/recsys_data/dssm_improved.json /Users/nadys/PythonProjects/recsys-course-spring-2024/botify/data/recommendations_dssm_improved.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-course-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

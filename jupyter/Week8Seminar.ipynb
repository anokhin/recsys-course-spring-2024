{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import lightfm\n",
    "import lightfm.data as ld\n",
    "import lightfm.evaluation as lv\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "import optuna\n",
    "\n",
    "import tensorboardX as tb\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные к семинару: https://cloud.mail.ru/public/jhPc/599CfjuEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    pd.read_json(data_path, lines=True) \n",
    "    for data_path \n",
    "    in glob.glob(\"./data/*/data.json\")\n",
    "])\n",
    "data[\"rnd\"] = np.random.random(len(data))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positives = data[data[\"time\"] > 0.8].copy()\n",
    "positives[\"test\"] = np.random.random(len(positives)) >= 0.7\n",
    "positives.drop_duplicates([\"user\", \"track\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_counts = positives[~positives[\"test\"]].groupby(\"user\").size()\n",
    "users = set(user_counts[user_counts >= 5].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "track_counts = positives[~positives[\"test\"]].groupby(\"track\").size()\n",
    "tracks = set(track_counts[track_counts >= 5].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users), len(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = positives[~positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "test_data = positives[positives[\"test\"] & positives[\"user\"].isin(users) & positives[\"track\"].isin(tracks)]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ld.Dataset()\n",
    "dataset.fit(users, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_interactions, _ = dataset.build_interactions(train_data[[\"user\", \"track\"]].itertuples(index=False, name=None))\n",
    "test_interactions, _ = dataset.build_interactions(test_data[[\"user\", \"track\"]].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "    epochs=1, \n",
    "    at=10,\n",
    "    loss=\"warp\",\n",
    "    no_components=30,\n",
    "    learning_rate=0.01, \n",
    "    max_sampled=10,\n",
    "    user_alpha=0.0, \n",
    "    item_alpha=0.0, \n",
    "    threads=30, \n",
    "    verbose=False,\n",
    "    patience=3,\n",
    "    epsilon=1e-6,\n",
    "):\n",
    "    model = lightfm.LightFM(\n",
    "        no_components=no_components,\n",
    "        loss=loss,\n",
    "        learning_rate=learning_rate,\n",
    "        max_sampled=max_sampled,\n",
    "        user_alpha=user_alpha,\n",
    "        item_alpha=item_alpha,\n",
    "    )\n",
    "\n",
    "    precisions_at = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model = model.fit_partial(train_interactions, num_threads=threads)\n",
    "        \n",
    "        precision_at = lv.precision_at_k(model, test_interactions, train_interactions=train_interactions, k=at, num_threads=threads)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{epoch}:\\t{np.mean(precision_at)} +/- {ss.sem(precision_at) * 1.96}\")\n",
    "            \n",
    "        precisions_at.append(np.mean(precision_at))\n",
    "            \n",
    "        if epoch > patience and all([precisions_at[-j] - precisions_at[-patience-1] < epsilon for j in range(1, patience + 1)]):\n",
    "            if verbose:\n",
    "                print(\"Early stopiing!\")\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No early stopiing happened: increase epochs maybe?\")\n",
    "        \n",
    "    return model, precisions_at\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    loss = trial.suggest_categorical(\"loss\", [\"warp\", \"bpr\"])\n",
    "    no_components = trial.suggest_categorical(\"no_components\", [10, 30, 50])\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.0001, 0.001, 0.01])\n",
    "    max_sampled = trial.suggest_categorical(\"max_sampled\", [10, 20, 50, 100])\n",
    "    user_alpha = trial.suggest_categorical(\"user_alpha\", [0.0, 0.0001])\n",
    "    item_alpha = trial.suggest_categorical(\"item_alpha\", [0.0, 0.0001])\n",
    "    \n",
    "    model, precisions_at = fit_model(\n",
    "        epochs=5, \n",
    "        at=10,\n",
    "        loss=loss,\n",
    "        no_components=no_components, \n",
    "        learning_rate=learning_rate, \n",
    "        max_sampled=max_sampled, \n",
    "        user_alpha=user_alpha, \n",
    "        item_alpha=item_alpha,\n",
    "    )\n",
    "    \n",
    "    return precisions_at[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=30)\n",
    "# best_params = study.best_params\n",
    "\n",
    "best_params = {\n",
    "    'loss': 'warp',\n",
    "    'no_components': 50,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_sampled': 100,\n",
    "    'user_alpha': 0.0,\n",
    "    'item_alpha': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, precisions_at = fit_model(\n",
    "    epochs=300,\n",
    "    at=10,\n",
    "    loss=best_params[\"loss\"],\n",
    "    no_components=best_params[\"no_components\"], \n",
    "    learning_rate=best_params[\"learning_rate\"], \n",
    "    max_sampled=best_params[\"max_sampled\"],\n",
    "    user_alpha=best_params[\"user_alpha\"],\n",
    "    item_alpha=best_params[\"item_alpha\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = pl.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(precisions_at)), precisions_at)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save track embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biases, embeddings = model.get_item_representations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "track_meta = pd.read_json(\"./data/tracks.json\", lines=True)\n",
    "track_meta[\"dataset_index\"] = track_meta[\"track\"].map(lambda t: dataset.mapping()[2].get(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_tracks = track_meta[pd.notnull(track_meta[\"dataset_index\"])].sort_values(\"dataset_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute top recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "max_tracks_from_same_artist = 5\n",
    "\n",
    "with open(f\"recommendations_{k}_{max_tracks_from_same_artist}.json\", \"w\") as rf:\n",
    "    for _, track in tqdm.tqdm(track_meta.iterrows()):\n",
    "        j = track[\"dataset_index\"]\n",
    "        \n",
    "        recommendations = []\n",
    "        if pd.notna(j):\n",
    "            embedding = embeddings[int(j)]\n",
    "            neighbours = np.argsort(-np.dot(embeddings, embedding))\n",
    "            \n",
    "            artists = defaultdict(int)\n",
    "            for neighbour in neighbours:\n",
    "                recommended_track = dataset_tracks[dataset_tracks[\"dataset_index\"] == neighbour].iloc[0]\n",
    "                \n",
    "                recommendation = int(recommended_track[\"track\"])\n",
    "                if recommendation == track[\"track\"]:\n",
    "                    continue\n",
    "                \n",
    "                artist = recommended_track[\"artist\"]\n",
    "                if artists[artist] >= max_tracks_from_same_artist:\n",
    "                    continue\n",
    "                \n",
    "                recommendations.append(recommendation)\n",
    "                artists[artist] += 1\n",
    "\n",
    "                if len(recommendations) == k:\n",
    "                    break\n",
    "         \n",
    "        track_with_recommendations = dict(track)\n",
    "        track_with_recommendations[\"recommendations\"] = recommendations\n",
    "        \n",
    "        rf.write(json.dumps(track_with_recommendations) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique artist per recommendation list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_track_artists = dict(zip(\n",
    "    dataset_tracks[\"track\"].values.tolist(),\n",
    "    dataset_tracks[\"artist\"].values.tolist(),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_json(f\"recommendations_{k}_{max_tracks_from_same_artist}.json\", lines=True)\n",
    "recs = recs[recs[\"dataset_index\"].notnull()]\n",
    "\n",
    "sample = recs.sample(frac=0.1).iloc[0]\n",
    "\n",
    "print(sample[\"title\"], \"by\" , sample[\"artist\"], \"\\n===\")\n",
    "print(\"\\n\".join([dataset_track_artists[track] for track in sample[\"recommendations\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_artists(tracks):\n",
    "    return len(\n",
    "        set([dataset_track_artists[track] for track in tracks])\n",
    "    )\n",
    "\n",
    "\n",
    "artist_counts = recs[\"recommendations\"].map(count_artists)\n",
    "artist_counts.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
